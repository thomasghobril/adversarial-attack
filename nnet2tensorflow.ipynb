{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation of ****.nnet*** (Stanford) file into ****.pb*** file (TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np \n",
    "import sys\n",
    "from tensorflow.python.framework import graph_util\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tom01\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Define readNNet, normalizeNNet, writeNNET functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNNet(nnetFile, withNorm=False):\n",
    "    '''\n",
    "    Read a .nnet file and return list of weight matrices and bias vectors\n",
    "    \n",
    "    Inputs:\n",
    "        nnetFile: (string) .nnet file to read\n",
    "        withNorm: (bool) If true, return normalization parameters\n",
    "    \n",
    "    Returns: \n",
    "        weights: List of weight matrices for fully connected network\n",
    "        biases: List of bias vectors for fully connected network\n",
    "    '''        \n",
    "    # Open NNet file\n",
    "    f = open(nnetFile,'r')\n",
    "    \n",
    "    # Skip header lines\n",
    "    line = f.readline()\n",
    "    while line[:2]==\"//\":\n",
    "        line = f.readline()\n",
    "        \n",
    "    # Extract information about network architecture\n",
    "    record = line.split(',')\n",
    "    numLayers   = int(record[0])\n",
    "    inputSize   = int(record[1])\n",
    "\n",
    "    line = f.readline()\n",
    "    record = line.split(',')\n",
    "    layerSizes = np.zeros(numLayers+1,'int')\n",
    "    for i in range(numLayers+1):\n",
    "        layerSizes[i]=int(record[i])\n",
    "\n",
    "    # Skip extra obsolete parameter line\n",
    "    f.readline()\n",
    "    \n",
    "    # Read the normalization information\n",
    "    line = f.readline()\n",
    "    inputMins = [float(x) for x in line.strip().split(\",\") if x]\n",
    "\n",
    "    line = f.readline()\n",
    "    inputMaxes = [float(x) for x in line.strip().split(\",\") if x]\n",
    "\n",
    "    line = f.readline()\n",
    "    means = [float(x) for x in line.strip().split(\",\") if x]\n",
    "\n",
    "    line = f.readline()\n",
    "    ranges = [float(x) for x in line.strip().split(\",\") if x]\n",
    "\n",
    "    # Read weights and biases\n",
    "    weights=[]\n",
    "    biases = []\n",
    "    for layernum in range(numLayers):\n",
    "\n",
    "        previousLayerSize = layerSizes[layernum]\n",
    "        currentLayerSize = layerSizes[layernum+1]\n",
    "        weights.append([])\n",
    "        biases.append([])\n",
    "        weights[layernum] = np.zeros((currentLayerSize,previousLayerSize))\n",
    "        for i in range(currentLayerSize):\n",
    "            line=f.readline()\n",
    "            aux = [float(x) for x in line.strip().split(\",\")[:-1]]\n",
    "            for j in range(previousLayerSize):\n",
    "                weights[layernum][i,j] = aux[j]\n",
    "        #biases\n",
    "        biases[layernum] = np.zeros(currentLayerSize)\n",
    "        for i in range(currentLayerSize):\n",
    "            line=f.readline()\n",
    "            x = float(line.strip().split(\",\")[0])\n",
    "            biases[layernum][i] = x\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    if withNorm:\n",
    "        return weights, biases, inputMins, inputMaxes, means, ranges\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeNNet(readNNetFile, writeNNetFile=None):\n",
    "    weights, biases, inputMins, inputMaxes, means, ranges = readNNet(readNNetFile,withNorm=True)\n",
    "    \n",
    "    numInputs = weights[0].shape[1]\n",
    "    numOutputs = weights[-1].shape[0]\n",
    "    \n",
    "    # Adjust weights and biases of first layer\n",
    "    for i in range(numInputs): weights[0][:,i]/=ranges[i]\n",
    "    biases[0]-= np.matmul(weights[0],means[:-1])\n",
    "    \n",
    "    # Adjust weights and biases of last layer\n",
    "    weights[-1]*=ranges[-1]\n",
    "    biases[-1] *= ranges[-1]\n",
    "    biases[-1] += means[-1]\n",
    "    \n",
    "    # Nominal mean and range vectors\n",
    "    means = np.zeros(numInputs+1)\n",
    "    ranges = np.ones(numInputs+1)\n",
    "    \n",
    "    if writeNNetFile is not None:\n",
    "        writeNNet(weights,biases,inputMins,inputMaxes,means,ranges,writeNNetFile)\n",
    "        return None\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeNNet(weights,biases,inputMins,inputMaxes,means,ranges,fileName):\n",
    "    '''\n",
    "    Write network data to the .nnet file format\n",
    "    Args:\n",
    "        weights (list): Weight matrices in the network order \n",
    "        biases (list): Bias vectors in the network order\n",
    "        inputMins (list): Minimum values for each input\n",
    "        inputMaxes (list): Maximum values for each input\n",
    "        means (list): Mean values for each input and a mean value for all outputs. Used to normalize inputs/outputs\n",
    "        ranges (list): Range values for each input and a range value for all outputs. Used to normalize inputs/outputs\n",
    "        fileName (str): File where the network will be written\n",
    "    '''\n",
    "    #Open the file we wish to write\n",
    "    with open(fileName,'w') as f2:\n",
    "\n",
    "        #####################\n",
    "        # First, we write the header lines:\n",
    "        # The first line written is just a line of text\n",
    "        # The second line gives the four values:\n",
    "        #     Number of fully connected layers in the network\n",
    "        #     Number of inputs to the network\n",
    "        #     Number of outputs from the network\n",
    "        #     Maximum size of any hidden layer\n",
    "        # The third line gives the sizes of each layer, including the input and output layers\n",
    "        # The fourth line gives an outdated flag, so this can be ignored\n",
    "        # The fifth line specifies the minimum values each input can take\n",
    "        # The sixth line specifies the maximum values each input can take\n",
    "        #     Inputs passed to the network are truncated to be between this range\n",
    "        # The seventh line gives the mean value of each input and of all outputs\n",
    "        # The eighth line gives the range of each input and of all outputs\n",
    "        #     These two lines are used to map raw inputs to the 0 mean, unit range of the inputs and outputs\n",
    "        #     used during training\n",
    "        # The ninth line begins the network weights and biases\n",
    "        ####################\n",
    "        f2.write(\"// Neural Network File Format by Kyle Julian, Stanford 2016\\n\")\n",
    "\n",
    "        #Extract the necessary information and write the header information\n",
    "        numLayers = len(weights)\n",
    "        inputSize = weights[0].shape[1]\n",
    "        outputSize = len(biases[-1])\n",
    "        maxLayerSize = inputSize\n",
    "        \n",
    "        # Find maximum size of any hidden layer\n",
    "        for b in biases:\n",
    "            if len(b)>maxLayerSize :\n",
    "                maxLayerSize = len(b)\n",
    "\n",
    "        # Write data to header \n",
    "        f2.write(\"%d,%d,%d,%d,\\n\" % (numLayers,inputSize,outputSize,maxLayerSize) )\n",
    "        f2.write(\"%d,\" % inputSize )\n",
    "        for b in biases:\n",
    "            f2.write(\"%d,\" % len(b) )\n",
    "        f2.write(\"\\n\")\n",
    "        f2.write(\"0,\\n\") #Unused Flag\n",
    "\n",
    "        # Write Min, Max, Mean, and Range of each of the inputs and outputs for normalization\n",
    "        f2.write(','.join(str(inputMins[i])  for i in range(inputSize)) + ',\\n') #Minimum Input Values\n",
    "        f2.write(','.join(str(inputMaxes[i]) for i in range(inputSize)) + ',\\n') #Maximum Input Values                \n",
    "        f2.write(','.join(str(means[i])      for i in range(inputSize+1)) + ',\\n') #Means for normalizations\n",
    "        f2.write(','.join(str(ranges[i])     for i in range(inputSize+1)) + ',\\n') #Ranges for noramlizations\n",
    "\n",
    "        ##################\n",
    "        # Write weights and biases of neural network\n",
    "        # First, the weights from the input layer to the first hidden layer are written\n",
    "        # Then, the biases of the first hidden layer are written\n",
    "        # The pattern is repeated by next writing the weights from the first hidden layer to the second hidden layer,\n",
    "        # followed by the biases of the second hidden layer.\n",
    "        ##################\n",
    "        for w,b in zip(weights,biases):\n",
    "            for i in range(w.shape[0]):\n",
    "                for j in range(w.shape[1]):\n",
    "                    f2.write(\"%.5e,\" % w[i][j]) #Five digits written. More can be used, but that requires more more space.\n",
    "                f2.write(\"\\n\")\n",
    "                \n",
    "            for i in range(len(b)):\n",
    "                f2.write(\"%.5e,\\n\" % b[i]) #Five digits written. More can be used, but that requires more more space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Define transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnet2pb(nnetFile, pbFile=\"\", output_node_names = \"y_out\", normalizeNetwork=False):\n",
    "    '''\n",
    "    Read a .nnet file and create a frozen Tensorflow graph and save to a .pb file\n",
    "    \n",
    "    Args:\n",
    "        nnetFile (str): A .nnet file to convert to Tensorflow format\n",
    "        pbFile (str, optional): Name for the created .pb file. Default: \"\"\n",
    "        output_node_names (str, optional): Name of the final operation in the Tensorflow graph. Default: \"y_out\"\n",
    "    '''\n",
    "    if normalizeNetwork:\n",
    "        weights, biases = normalizeNNet(nnetFile)\n",
    "    else:\n",
    "        weights, biases = readNNet(nnetFile)\n",
    "    inputSize = weights[0].shape[1]\n",
    "    \n",
    "    # Default pb filename if none are specified\n",
    "    if pbFile==\"\":\n",
    "        pbFile = nnetFile[:-4]+'pb'\n",
    "    \n",
    "    # Reset tensorflow and load a session using only CPUs\n",
    "    # tf.compat.v1.reset_default_graph() ### COMPAT.V1.\n",
    "    # sess = tf.compat.v1.Session() ### COMPAT.V1.\n",
    "    tf.reset_default_graph() ### COMPAT.V1.\n",
    "    sess = tf.Session() ### COMPAT.V1.\n",
    "\n",
    "    # Define model and assign values to tensors\n",
    "    currentTensor = tf.placeholder(tf.float32, [None, inputSize],name='input')\n",
    "    for i in range(len(weights)):\n",
    "        W = tf.get_variable(\"W%d\"%i, shape=weights[i].T.shape)\n",
    "        b = tf.get_variable(\"b%d\"%i, shape=biases[i].shape)\n",
    "        \n",
    "        # Use ReLU for all but last operation, and name last operation to desired name\n",
    "        if i!=len(weights)-1:\n",
    "            currentTensor = tf.nn.relu(tf.matmul(currentTensor ,W) + b)\n",
    "        else:\n",
    "            currentTensor =  tf.add(tf.matmul(currentTensor ,W), b,name=output_node_names)\n",
    "\n",
    "        # Assign values to tensors\n",
    "        sess.run(tf.assign(W,weights[i].T))\n",
    "        sess.run(tf.assign(b,biases[i]))\n",
    "    \n",
    "    # Freeze the graph to write the pb file\n",
    "    freeze_graph(sess,pbFile,output_node_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(sess, output_graph_name, output_node_names):\n",
    "    '''\n",
    "    Given a session with a graph loaded, save only the variables needed for evaluation to a .pb file\n",
    "    \n",
    "    Args:\n",
    "        sess (tf.session): Tensorflow session where graph is defined\n",
    "        output_graph_name (str): Name of file for writing frozen graph\n",
    "        output_node_names (str): Name of the output operation in the graph, comma separated if there are multiple output operations\n",
    "    '''\n",
    "    \n",
    "    input_graph_def = tf.get_default_graph().as_graph_def()\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess,                        # The session is used to retrieve the weights\n",
    "        input_graph_def,             # The graph_def is used to retrieve the nodes \n",
    "        output_node_names.split(\",\") # The output node names are used to select the useful nodes\n",
    "    ) \n",
    "\n",
    "    # Finally we serialize and dump the output graph to the file\n",
    "    with tf.gfile.GFile(output_graph_name, \"w\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Transform to *.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-2a56723c9cda>:12: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From C:\\Users\\tom01\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:856: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "> TF network saved as *.pb file !\n"
     ]
    }
   ],
   "source": [
    "# Read user inputs and run writePB function\n",
    "\n",
    "nnetFile = \"ACAS_Xu_NNet/\" + \"ACASXU_1_1.nnet\"\n",
    "\n",
    "nnet2pb(nnetFile,\"\",\"y_out\")\n",
    "print(\"> TF network saved as *.pb file !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Transform in *.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACAS_Xu_NNet/ACASXU_1_1.pb\n"
     ]
    }
   ],
   "source": [
    "tfFile = nnetFile[:-4] + \"pb\"\n",
    "print(tfFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: saved_model.pb/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-dd197a326c44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved_model.pb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    109\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n\u001b[0m\u001b[0;32m    112\u001b[0m                   (export_dir,\n\u001b[0;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: saved_model.pb/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"saved_model.pb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
